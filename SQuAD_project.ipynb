{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-X-SQAD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6J9_LsBSwk-"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwEFNn5KuGDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd6634f-d99d-496e-a00d-86fec69da7cb"
      },
      "source": [
        "!pip install -q tf-models-official==2.3.0\r\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxC2Vn2KR8pK"
      },
      "source": [
        "import os\r\n",
        "import json\r\n",
        "\r\n",
        "import re\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "tfds.disable_progress_bar()\r\n",
        "\r\n",
        "from official.modeling import tf_utils\r\n",
        "from official import nlp\r\n",
        "from official.nlp import bert\r\n",
        "\r\n",
        "# Load the required submodules\r\n",
        "import official.nlp.optimization\r\n",
        "import official.nlp.bert.bert_models\r\n",
        "import official.nlp.bert.configs\r\n",
        "import official.nlp.bert.run_classifier\r\n",
        "import official.nlp.bert.tokenization\r\n",
        "import official.nlp.data.classifier_data_lib\r\n",
        "import official.nlp.modeling.losses\r\n",
        "import official.nlp.modeling.models\r\n",
        "import official.nlp.modeling.networks"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85YO229Wu0iJ"
      },
      "source": [
        "This directory contains the configuration, vocabulary, and a pre-trained checkpoint used in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-lQVnxBu27d",
        "outputId": "a2bb9d98-3f48-45a5-91c6-e70bfc7b3698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\r\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8-XneWru53K"
      },
      "source": [
        "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A23pEgzInJLM"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwEW25ifc3Pt"
      },
      "source": [
        "# import sys\r\n",
        "# !git clone \"https://github.com/giuliofortini/NLP_SQuAD_Project\"\r\n",
        "# sys.path.append(\"NLP_SQuAD_Project/\")\r\n",
        "\r\n",
        "from google.colab import files\r\n",
        "try:\r\n",
        "  with open('training_set.json') as f:\r\n",
        "    json_data = json.load(f)\r\n",
        "except:\r\n",
        "  files.upload()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o1FPMIyWWgw",
        "outputId": "00dc0c6c-c944-4a06-e230-7b2a6bc94098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data = pd.json_normalize(json_data['data'])  \r\n",
        "#data = pd.json_normalize(json_data['data'], ['paragraphs'])\r\n",
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>paragraphs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>[{'context': 'Architecturally, the school has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>[{'context': 'Beyoncé Giselle Knowles-Carter (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Montana</td>\n",
              "      <td>[{'context': 'Montana i/mɒnˈtænə/ is a state i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Genocide</td>\n",
              "      <td>[{'context': 'The phrase \"in whole or in part\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Antibiotics</td>\n",
              "      <td>[{'context': 'The emergence of resistance of b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>Police</td>\n",
              "      <td>[{'context': 'A police force is a constituted ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>Punjab,_Pakistan</td>\n",
              "      <td>[{'context': 'Punjab (Urdu, Punjabi: پنجاب, pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>Infection</td>\n",
              "      <td>[{'context': ' Among the vast varieties of mic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>Hunting</td>\n",
              "      <td>[{'context': 'Hunting is the practice of killi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>[{'context': 'Kathmandu(/ˌkɑːtmɑːnˈduː/; Nepal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>442 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        title                                         paragraphs\n",
              "0    University_of_Notre_Dame  [{'context': 'Architecturally, the school has ...\n",
              "1                     Beyoncé  [{'context': 'Beyoncé Giselle Knowles-Carter (...\n",
              "2                     Montana  [{'context': 'Montana i/mɒnˈtænə/ is a state i...\n",
              "3                    Genocide  [{'context': 'The phrase \"in whole or in part\"...\n",
              "4                 Antibiotics  [{'context': 'The emergence of resistance of b...\n",
              "..                        ...                                                ...\n",
              "437                    Police  [{'context': 'A police force is a constituted ...\n",
              "438          Punjab,_Pakistan  [{'context': 'Punjab (Urdu, Punjabi: پنجاب, pa...\n",
              "439                 Infection  [{'context': ' Among the vast varieties of mic...\n",
              "440                   Hunting  [{'context': 'Hunting is the practice of killi...\n",
              "441                 Kathmandu  [{'context': 'Kathmandu(/ˌkɑːtmɑːnˈduː/; Nepal...\n",
              "\n",
              "[442 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc6-zf6cV4e6",
        "outputId": "600803ee-4fb9-4ce4-f04c-d14229934e6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "temp = []\r\n",
        "title_dict = {}\r\n",
        "contexts = []\r\n",
        "\r\n",
        "for i, row in data.iterrows():\r\n",
        "  for context in row['paragraphs']:\r\n",
        "    contexts.append(context['context'])\r\n",
        "    for qa in context['qas']:\r\n",
        "      question_id = qa['id']\r\n",
        "      question = qa['question']\r\n",
        "      for answer in qa['answers']:\r\n",
        "        answer_text = answer['text']\r\n",
        "        answer_start = answer['answer_start']\r\n",
        "        answer_end = answer_start+len(answer_text)\r\n",
        "        temp.append([question_id, question, answer_text, answer_start, answer_end, i, len(contexts)-1])\r\n",
        "\r\n",
        "\r\n",
        "context_dict = dict(enumerate(contexts))\r\n",
        "\r\n",
        "df = pd.DataFrame(temp, columns=['question_id', 'question_text', 'answer_text', 'answer_start', 'answer_end', 'title_id', 'context_id'])\r\n",
        "#df['context_id'] = context_df.index\r\n",
        "context_dict[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtYH3UjM7M_6",
        "outputId": "2eb30c0e-9413-4e96-e4ea-6945738c590c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question_text</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>title_id</th>\n",
              "      <th>context_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>515</td>\n",
              "      <td>541</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>188</td>\n",
              "      <td>213</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>279</td>\n",
              "      <td>296</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>381</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>92</td>\n",
              "      <td>126</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87594</th>\n",
              "      <td>5735d259012e2f140011a09d</td>\n",
              "      <td>In what US state did Kathmandu first establish...</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>229</td>\n",
              "      <td>235</td>\n",
              "      <td>441</td>\n",
              "      <td>18895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87595</th>\n",
              "      <td>5735d259012e2f140011a09e</td>\n",
              "      <td>What was Yangon previously known as?</td>\n",
              "      <td>Rangoon</td>\n",
              "      <td>414</td>\n",
              "      <td>421</td>\n",
              "      <td>441</td>\n",
              "      <td>18895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87596</th>\n",
              "      <td>5735d259012e2f140011a09f</td>\n",
              "      <td>With what Belorussian city does Kathmandu have...</td>\n",
              "      <td>Minsk</td>\n",
              "      <td>476</td>\n",
              "      <td>481</td>\n",
              "      <td>441</td>\n",
              "      <td>18895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87597</th>\n",
              "      <td>5735d259012e2f140011a0a0</td>\n",
              "      <td>In what year did Kathmandu create its initial ...</td>\n",
              "      <td>1975</td>\n",
              "      <td>199</td>\n",
              "      <td>203</td>\n",
              "      <td>441</td>\n",
              "      <td>18895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87598</th>\n",
              "      <td>5735d259012e2f140011a0a1</td>\n",
              "      <td>What is KMC an initialism of?</td>\n",
              "      <td>Kathmandu Metropolitan City</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>441</td>\n",
              "      <td>18895</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87599 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    question_id  ... context_id\n",
              "0      5733be284776f41900661182  ...          0\n",
              "1      5733be284776f4190066117f  ...          0\n",
              "2      5733be284776f41900661180  ...          0\n",
              "3      5733be284776f41900661181  ...          0\n",
              "4      5733be284776f4190066117e  ...          0\n",
              "...                         ...  ...        ...\n",
              "87594  5735d259012e2f140011a09d  ...      18895\n",
              "87595  5735d259012e2f140011a09e  ...      18895\n",
              "87596  5735d259012e2f140011a09f  ...      18895\n",
              "87597  5735d259012e2f140011a0a0  ...      18895\n",
              "87598  5735d259012e2f140011a0a1  ...      18895\n",
              "\n",
              "[87599 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUcu-8VyTqs",
        "outputId": "f3454b74-ee9d-4c78-9b90-047e5ab4c5e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\r\n",
        "def print_squad_sample(train_data, line_length=120, separator_length=150):\r\n",
        "  sample = train_data.sample(frac=1).head(1)\r\n",
        "  context = context_dict[sample['context_id'].item()]\r\n",
        "  print('='*separator_length)\r\n",
        "  print('CONTEXT: ')\r\n",
        "  print('='*separator_length)\r\n",
        "  lines = [''.join(context[idx:idx+line_length]) for idx in range(0, len(context), line_length)]\r\n",
        "  for l in lines:\r\n",
        "      print(l)\r\n",
        "  #print(context)\r\n",
        "  print('='*separator_length)\r\n",
        "  questions = train_data[train_data['context_id'] == sample['context_id'].item()]\r\n",
        "  print('QUESTION:', ' '*(3*separator_length//4), 'ANSWER:')\r\n",
        "  for idx, row in questions.iterrows():\r\n",
        "    question = row.question_text\r\n",
        "    answer = row.answer_text\r\n",
        "    print(question, ' '*(3*separator_length//4-len(question)+9), answer)\r\n",
        "\r\n",
        "print_squad_sample(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======================================================================================================================================================\n",
            "CONTEXT: \n",
            "======================================================================================================================================================\n",
            "In 1409, a hospital was founded and placed under the patronage of Santa María de los Inocentes; to this was attached a c\n",
            "onfraternity devoted to recovering the bodies of the unfriended dead in the city and within a radius of three miles (4.8\n",
            " km) around it. At the end of the 15th century this confraternity separated from the hospital, and continued its work un\n",
            "der the name of \"Cofradia para el ámparo de los desamparados\". King Philip IV of Spain and the Duke of Arcos suggested t\n",
            "he building of the new chapel, and in 1647 the Viceroy, Conde de Oropesa, who had been preserved from the bubonic plague\n",
            ", insisted on carrying out their project. The Blessed Virgin was proclaimed patroness of the city under the title of Vir\n",
            "gen de los desamparados (Virgin of the Forsaken), and Archbishop Pedro de Urbina, on 31 June 1652, laid the cornerstone \n",
            "of the new chapel of this name. The archiepiscopal palace, a grain market in the time of the Moors, is simple in design,\n",
            " with an inside cloister and a handsome chapel. In 1357, the arch that connects it with the cathedral was built. In the \n",
            "council chamber are preserved the portraits of all the prelates of Valencia.\n",
            "======================================================================================================================================================\n",
            "QUESTION:                                                                                                                  ANSWER:\n",
            "Who was the hospital's patroness?                                                                                          Santa María de los Inocentes\n",
            "What was the confraternity called after its separation from the hospital?                                                  Cofradia para el ámparo de los desamparados\n",
            "Who placed the chapel's cornerstone?                                                                                       Archbishop Pedro de Urbina\n",
            "What was the archiepiscopal palace during the Moorish period?                                                              a grain market\n",
            "Who insisted on building the new chapel?                                                                                   Conde de Oropesa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAzY4ocznL8v"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqrIFmrK2Igt"
      },
      "source": [
        "#Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMp8rs-PG_Ju",
        "outputId": "2aad8525-9a8e-40ab-bbad-094922fb7a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\r\n",
        "tokenizer = bert.tokenization.FullTokenizer(\r\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\r\n",
        "     do_lower_case=True)\r\n",
        "\r\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wtE6gqlmeSo",
        "outputId": "a3925807-5c9e-4678-981c-0c5b6eeee365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize our sentence with the BERT tokenizer.\r\n",
        "tokens = tokenizer.tokenize(df['question_text'][0])\r\n",
        "\r\n",
        "# Print out the tokens.\r\n",
        "print(tokens)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lou', '##rdes', 'france', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dOJEieLs-JG",
        "outputId": "1626cc59-d42b-45de-e669-26757cabb22d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avVC8QotmvgE",
        "outputId": "a396fc5e-c446-4631-a5f1-1b0932a4fe7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Split the sentence into tokens.\r\n",
        "tokenized_text = tokenizer.tokenize(df['question_text'][0])\r\n",
        "\r\n",
        "# Print out the tokens.\r\n",
        "print(tokenized_text)\r\n",
        "\r\n",
        "# Map the token strings to their vocabulary indeces.\r\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n",
        "\r\n",
        "# Display the words with their indeces.\r\n",
        "for tup in zip(tokenized_text, indexed_tokens):\r\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lou', '##rdes', 'france', '?']\n",
            "to            2,000\n",
            "whom          3,183\n",
            "did           2,106\n",
            "the           1,996\n",
            "virgin        6,261\n",
            "mary          2,984\n",
            "allegedly     9,382\n",
            "appear        3,711\n",
            "in            1,999\n",
            "1858          8,517\n",
            "in            1,999\n",
            "lou          10,223\n",
            "##rdes       26,371\n",
            "france        2,605\n",
            "?             1,029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjYSvcUtypc"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "import re\r\n",
        "\r\n",
        "# convert tokens to ids\r\n",
        "def tokens_to_int(column):\r\n",
        "  return tokenizer.convert_tokens_to_ids(column)\r\n",
        "\r\n",
        "# convert ids to tokens\r\n",
        "def int_to_token(id):\r\n",
        "  return tokenizer.convert_ids_to_tokens(id)\r\n",
        "\r\n",
        "\r\n",
        "# tokenize non-context column\r\n",
        "def tokenize_text(text):\r\n",
        "  return ['[CLS]'] + tokenizer.tokenize(text) + ['[SEP]']\r\n",
        "\r\n",
        "# tokenize context column\r\n",
        "def tokenize_context(idx):\r\n",
        "  context_dict[idx] = context_dict[idx].replace('.\"', '\".')\r\n",
        "  sentences = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', context_dict[idx].strip())\r\n",
        "  tokenized_sentence = []\r\n",
        "  for sentence in sentences:\r\n",
        "    tok = tokenizer.tokenize(sentence) + [\"[SEP]\"]\r\n",
        "    tokenized_sentence += tok\r\n",
        "  return tokenized_sentence #tokenizer.tokenize(re.sub('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', ' [SEP] ', context_dict[idx].strip())) + ['[SEP]']\r\n",
        "\r\n",
        "# create column mask\r\n",
        "def get_mask(tokenized_column, max_len):\r\n",
        "  mask = []\r\n",
        "  for row in tokenized_column:\r\n",
        "    mask.append(tf.concat((tf.zeros(max_len-len(row)), tf.ones_like(row, dtype=tf.float32)), axis=0))\r\n",
        "  return mask\r\n",
        "\r\n",
        "# return tokenized df with input masks\r\n",
        "def process_df(df):\r\n",
        "  \r\n",
        "  # initialize structures\r\n",
        "  columns = ['question_text', 'context'] #, 'answer_text']\r\n",
        "  processed_df = pd.DataFrame(columns=columns)\r\n",
        "  input_mask = []\r\n",
        "\r\n",
        "  for column in columns:\r\n",
        "    print(\"processing column: \", column)\r\n",
        "    \r\n",
        "    if column == 'question_text':\r\n",
        "      # tokenize column\r\n",
        "      tokenized_column = df[column].apply(tokenize_text).apply(tokens_to_int)\r\n",
        "      processed_df[column] = pad_sequences(tokenized_column, padding=\"post\").tolist()\r\n",
        "      # create input mask\r\n",
        "      max_len = len(processed_df[column][0])\r\n",
        "      input_mask = get_mask(tokenized_column, max_len)\r\n",
        "      processed_df[column+'_mask'] = input_mask\r\n",
        "    # elif column == 'answer_text':\r\n",
        "    #   # answer_text len must be equale to len(padded question_text) + len(padded context)\r\n",
        "    #   max_len = len(processed_df['question_text'][0]) + len(processed_df['context'][0])\r\n",
        "    #   # tokenize column\r\n",
        "    #   tokenized_column = df[column].apply(tokenize_text).apply(tokens_to_int)\r\n",
        "    #   processed_df[column] = pad_sequences(tokenized_column, max_len).tolist()\r\n",
        "    #   # create input mask\r\n",
        "    #   input_mask = get_mask(tokenized_column, max_len)\r\n",
        "    #   processed_df[column+'_mask'] = input_mask\r\n",
        "    else:\r\n",
        "      # tokenize column\r\n",
        "      tokenized_column = df['context_id'].apply(tokenize_context).apply(tokens_to_int)\r\n",
        "      processed_df[column] = pad_sequences(tokenized_column, padding=\"post\").tolist()\r\n",
        "      # create input mask\r\n",
        "      max_len = len(processed_df[column][0])\r\n",
        "      input_mask = get_mask(tokenized_column, max_len)\r\n",
        "      processed_df['context_mask'] = input_mask\r\n",
        "  \r\n",
        "  # add useful columns\r\n",
        "  processed_df['question_id'] = df['question_id']\r\n",
        "  processed_df['answer_start'] = df['answer_start']\r\n",
        "  processed_df['answer_end'] = df['answer_end']\r\n",
        "  \r\n",
        "  return processed_df\r\n",
        "\r\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFCJzAklAQHW",
        "outputId": "6f297735-b2e9-4e47-a647-f350a7b14e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "context_bkp = context_dict.copy()\r\n",
        "context_bkp[0]"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSrk7Zei8q3n"
      },
      "source": [
        "# CONTEXT_LIMIT = 2300\r\n",
        "# mx = 0\r\n",
        "# for id in context_dict:\r\n",
        "#   if len(context_dict[id]) > mx: mx = len(context_dict[id])\r\n",
        "#   context_dict[id] = context_dict[id][:CONTEXT_LIMIT]\r\n",
        "\r\n",
        "# print(\"max: \", mx)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9pPpMHFowE9",
        "outputId": "9aa4fe4c-153f-48d2-dab7-c20ab8bdc2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        }
      },
      "source": [
        "processed_df = process_df(df)\r\n",
        "processed_df"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing column:  question_text\n",
            "processing column:  context\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>context</th>\n",
              "      <th>question_text_mask</th>\n",
              "      <th>context_mask</th>\n",
              "      <th>question_id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382...</td>\n",
              "      <td>[6549, 2135, 1010, 1996, 2082, 2038, 1037, 323...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>515</td>\n",
              "      <td>541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[101, 2054, 2003, 1999, 2392, 1997, 1996, 1028...</td>\n",
              "      <td>[6549, 2135, 1010, 1996, 2082, 2038, 1037, 323...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>188</td>\n",
              "      <td>213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[101, 1996, 13546, 1997, 1996, 6730, 2540, 201...</td>\n",
              "      <td>[6549, 2135, 1010, 1996, 2082, 2038, 1037, 323...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>279</td>\n",
              "      <td>296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[101, 2054, 2003, 1996, 24665, 23052, 2012, 10...</td>\n",
              "      <td>[6549, 2135, 1010, 1996, 2082, 2038, 1037, 323...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>381</td>\n",
              "      <td>420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[101, 2054, 7719, 2006, 2327, 1997, 1996, 2364...</td>\n",
              "      <td>[6549, 2135, 1010, 1996, 2082, 2038, 1037, 323...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>92</td>\n",
              "      <td>126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87594</th>\n",
              "      <td>[101, 1999, 2054, 2149, 2110, 2106, 28045, 203...</td>\n",
              "      <td>[28045, 4956, 2103, 1006, 2463, 2278, 1007, 10...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5735d259012e2f140011a09d</td>\n",
              "      <td>229</td>\n",
              "      <td>235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87595</th>\n",
              "      <td>[101, 2054, 2001, 8675, 2239, 3130, 2124, 2004...</td>\n",
              "      <td>[28045, 4956, 2103, 1006, 2463, 2278, 1007, 10...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5735d259012e2f140011a09e</td>\n",
              "      <td>414</td>\n",
              "      <td>421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87596</th>\n",
              "      <td>[101, 2007, 2054, 19337, 21694, 17043, 2103, 2...</td>\n",
              "      <td>[28045, 4956, 2103, 1006, 2463, 2278, 1007, 10...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5735d259012e2f140011a09f</td>\n",
              "      <td>476</td>\n",
              "      <td>481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87597</th>\n",
              "      <td>[101, 1999, 2054, 2095, 2106, 28045, 3443, 204...</td>\n",
              "      <td>[28045, 4956, 2103, 1006, 2463, 2278, 1007, 10...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5735d259012e2f140011a0a0</td>\n",
              "      <td>199</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87598</th>\n",
              "      <td>[101, 2054, 2003, 2463, 2278, 2019, 3988, 2964...</td>\n",
              "      <td>[28045, 4956, 2103, 1006, 2463, 2278, 1007, 10...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>(tf.Tensor(0.0, shape=(), dtype=float32), tf.T...</td>\n",
              "      <td>5735d259012e2f140011a0a1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87599 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           question_text  ... answer_end\n",
              "0      [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382...  ...        541\n",
              "1      [101, 2054, 2003, 1999, 2392, 1997, 1996, 1028...  ...        213\n",
              "2      [101, 1996, 13546, 1997, 1996, 6730, 2540, 201...  ...        296\n",
              "3      [101, 2054, 2003, 1996, 24665, 23052, 2012, 10...  ...        420\n",
              "4      [101, 2054, 7719, 2006, 2327, 1997, 1996, 2364...  ...        126\n",
              "...                                                  ...  ...        ...\n",
              "87594  [101, 1999, 2054, 2149, 2110, 2106, 28045, 203...  ...        235\n",
              "87595  [101, 2054, 2001, 8675, 2239, 3130, 2124, 2004...  ...        421\n",
              "87596  [101, 2007, 2054, 19337, 21694, 17043, 2103, 2...  ...        481\n",
              "87597  [101, 1999, 2054, 2095, 2106, 28045, 3443, 204...  ...        203\n",
              "87598  [101, 2054, 2003, 2463, 2278, 2019, 3988, 2964...  ...         27\n",
              "\n",
              "[87599 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33VKjuJEkaKr",
        "outputId": "3584faf6-8480-4867-fc32-977adadef019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(processed_df['context'][0]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhE2kVlyurep",
        "outputId": "5f132377-5f50-4cb5-f14e-98a6649b9df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "question_len = len(processed_df['question_text_mask'][0])\r\n",
        "context_len = len(processed_df['context_mask'][0])\r\n",
        "input_len = question_len + context_len\r\n",
        "#answer_len = len(processed_df['answer_text_mask'][0])\r\n",
        "\r\n",
        "print(\"question_text_mask len: \", question_len)\r\n",
        "print(\"context_mask len: \", context_len)\r\n",
        "print(\"input len: \", input_len)\r\n",
        "#print(\"answer_text_mask len: \", answer_len)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question_text_mask len:  63\n",
            "context_mask len:  880\n",
            "input len:  943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0vFcRy5XwQ1",
        "outputId": "51320093-224f-4100-9c86-148259140898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def preprocess_input(processed_df):\r\n",
        "  \r\n",
        "  # initialize structures\r\n",
        "  input = []  \r\n",
        "  input_mask = []  \r\n",
        "  input_type_ids = []\r\n",
        "\r\n",
        "  print(\"\\nprocessing input, input_type_ids and input_mask...\\n\")\r\n",
        "  for i, row in processed_df.iterrows():\r\n",
        "\r\n",
        "    if i % 10000 == 0:\r\n",
        "      print(i)\r\n",
        "    \r\n",
        "    # create input tensor by concatenation of question_text and context\r\n",
        "    input.append(tf.convert_to_tensor(row['question_text'] + \r\n",
        "                                      row['context'] + \r\n",
        "                                      [row['answer_start']] + \r\n",
        "                                      [row['answer_end']] ))\r\n",
        "\r\n",
        "    # create type_ids tensor (needed for model input)\r\n",
        "    type_question = tf.zeros_like(row['question_text'])\r\n",
        "    type_context = tf.zeros_like(row['context'])\r\n",
        "    type_answer_idxs = tf.ones(2, dtype=tf.int32)\r\n",
        "    input_type_ids.append(tf.concat([type_question, type_context, type_answer_idxs], axis=-1))\r\n",
        "\r\n",
        "    # create input attention mask based on padding\r\n",
        "    input_mask.append(tf.concat((row['question_text_mask'], row['context_mask'], [1], [1]), axis=-1))\r\n",
        "\r\n",
        "  print(\"\\ncreating dictionary...\", end=\"\")\r\n",
        "  # save input data as dictionary\r\n",
        "  inputs = {\r\n",
        "    'input_ids': tf.convert_to_tensor(input),\r\n",
        "    'input_mask': tf.convert_to_tensor(input_mask),\r\n",
        "    'input_type_ids': tf.convert_to_tensor(input_type_ids)\r\n",
        "    }\r\n",
        "    \r\n",
        "  print(\"done!\\n\")\r\n",
        "  return inputs\r\n",
        "\r\n",
        "model_input = preprocess_input(processed_df)\r\n",
        "#model_labels = tf.convert_to_tensor(list(processed_df['answer_text']))\r\n",
        "\r\n",
        "print('input_ids:', model_input['input_ids'])\r\n",
        "print('input_mask: ', model_input['input_mask'])\r\n",
        "print('input_type_ids:', model_input['input_type_ids'])\r\n",
        "# print('labels: ', model_labels)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "processing input, input_type_ids and input_mask...\n",
            "\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "\n",
            "creating dictionary...done!\n",
            "\n",
            "input_ids: tf.Tensor(\n",
            "[[  101  2000  3183 ...     0   515   541]\n",
            " [  101  2054  2003 ...     0   188   213]\n",
            " [  101  1996 13546 ...     0   279   296]\n",
            " ...\n",
            " [  101  2007  2054 ...     0   476   481]\n",
            " [  101  1999  2054 ...     0   199   203]\n",
            " [  101  2054  2003 ...     0     0    27]], shape=(87599, 945), dtype=int32)\n",
            "input_mask:  tf.Tensor(\n",
            "[[0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]], shape=(87599, 945), dtype=float32)\n",
            "input_type_ids: tf.Tensor(\n",
            "[[0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]\n",
            " ...\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 1]], shape=(87599, 945), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGDtxXJIzt6a"
      },
      "source": [
        "# Set up epochs and steps\r\n",
        "epochs = 3\r\n",
        "batch_size = 32\r\n",
        "eval_batch_size = 32\r\n",
        "\r\n",
        "train_data_size = len(model_input['input_ids'])\r\n",
        "steps_per_epoch = int(train_data_size / batch_size)\r\n",
        "num_train_steps = steps_per_epoch * epochs\r\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\r\n",
        "\r\n",
        "# # creates an optimizer with learning rate schedule\r\n",
        "# optimizer = nlp.optimization.create_optimizer(\r\n",
        "#     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\r\n",
        "# metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\r\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AG7ZYbEUA8s",
        "outputId": "0320ce4c-b07c-414c-8dad-cd0a63e805e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import TFBertModel\r\n",
        "bert_pretrained = TFBertModel.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmJeTkqs75-Y",
        "outputId": "ac828693-f3c0-4917-fa25-a5b6f20ebb42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "tf.keras.utils.plot_model(bert_pretrained, show_shapes=True, dpi=48)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFEAAAAeCAYAAABUgfKPAAAABmJLR0QA/wD/AP+gvaeTAAADyUlEQVRoge2aT0gqURjFjyZRSYzlmJUmFBRUbqxNkbgLjYIisFZBQmCB1S6IIqpdLfpLUUabXFQWLaRdiwIXGW0iyCAUEpNAcxGWK+t7i/eSMntWb96LF/MDwev95pw7x5lx5l4FRETg+RO2hF89gu+A6OmNy+WC3+//yrH8V9TW1kKpVAIA4kfi1NTUlw3of8PlcuHw8DDeFj3vNBqN/3xA3wH+msgBfIgcwIfIAXyIHMCHyAEpQ7y/v8fGxka83d3djczMzKS1CoUC29vb7zZP1OaKubk5lJSUJO1rbm5GV1fXm9t6vV50dnZ+yC9liMfHx4jFYgAAIsLFxQWi0WjS2rGxMQgEgnebP9fmkr6+PpSVlSXtW1hYeHO7UCiEoaEhXF5efsgvZYiDg4Po6OjA7u4uent7sb+/D41G82a9yWSCWCzG8PAwAMDv90OtVkOpVOLs7AwGgwH19fVQq9UvtBNpaGgAwzBIT0+HXC6HRCJBbm4uPB4PAODq6grV1dVgGAaTk5Pxz6qqqpCRkRGvS/T/HTKZ7FNnRsoQzWYzbDYbmpqaMDMzA71ej5OTkzfr5+fn4fP5sLOzg0gkAqvVipGREWxubsJqtcJisaCurg7j4+MvtBPp7+/HwMAAQqEQVCoVAoEAenp6cHp6CgBYWlqC2WyGx+PB2toaotEoFhcXYTKZcHd3Fz+dE/3/Bpz/sIjFYrAsi8rKSoTDYQQCAbS3t0Or1SIQCAAAJBIJWltb36XFMAykUinEYjGKiorw+PgI4OcRptPpIJPJUFFRgWAwCJ/Ph8bGRohEIgiFP3ctmT/XvCvE29tbPDw8vEswFoshGAzC7XajoKAA+fn5WF9fRywWg81m+yPt56hUKjidToTDYbjdbuTl5UEul+Pg4ADhcBiRSAQAUvpzAv3CaDRSMo6Ojig7O5tWV1dpdHSURCIRTU9PJ611OByUk5NDDMPQ7OwsERF5vV4qLy8nlmVpb2+P9Ho9sSxL19fXL7QTMRgMJJfLaWVlhdLS0mh5eZkKCwtJq9USEZHf7yeNRkMMw9DExAQREZ2fn1NxcTEpFAqSSqXkcDhe+be0tFBWVhY5nc5Xnh6Ph0pLS0koFJJOp0u6j0REdrud7HZ7vCl4mpRta2uD3W7/O9/UN2NrawtAfMLmc5OyNzc3EAgEL14Wi+VTA+JS66s8RalLXsOyLLhaVeBS66s8+cc+DuBD5AA+RA7gQ+QAPkQO4EPkgBe3OE83kTy/x+VyoaamJt6OP7Hwi/cf49ni/VY8RJ5Pw/8Xhwt+AA2QK+BArGeFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_cChz6E6OEX",
        "outputId": "c387bbe4-1da6-4eb9-dca5-151691c192b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# compute the max distance between consequent 'id' occurrences\r\n",
        "def max_distance(arr, id):\r\n",
        "  positions = np.where(np.array(arr) == id)[0]\r\n",
        "  distances = [positions[i+1] - positions[i] - 1 for i in range(len(positions)-1)]\r\n",
        "  return max(distances)\r\n",
        "\r\n",
        "# find the too long sentence (having more than 512 tokens between two SEP token)\r\n",
        "count = 0\r\n",
        "sentences_lengths = []\r\n",
        "for i, row in enumerate(model_input[\"input_ids\"].numpy()):\r\n",
        "  dist = max_distance(row, 102)\r\n",
        "  if dist >= 512: # sentences longer than 512 are rejected by BERT\r\n",
        "    sentences_lengths.append((dist, i))\r\n",
        "    count += 1\r\n",
        "  # print(row)\r\n",
        "\r\n",
        "print(f\"Found {count} sentences longer than 512 tokens.\")\r\n",
        "\r\n",
        "if len(sentences_lengths) != 0:\r\n",
        "  max_length, longest_id = max(sentences_lengths)\r\n",
        "  toolong = context_dict[df[\"context_id\"][longest_id]]\r\n",
        "  tokenized_toolong = model_input[\"input_ids\"][longest_id].numpy()\r\n",
        "  tokenized_toolong = [t for t in tokenized_toolong]\r\n",
        "  tokenized_toolong = ' '.join(int_to_token(tokenized_toolong))\r\n",
        "  print(f\"Longest sentences's length: {max_length}\")\r\n",
        "  print(f\"\\nLongest sentence (id {longest_id}): \\n  {toolong}\")\r\n",
        "  print(f\"\\nTokenized: \\n  {tokenized_toolong}\\n\\n\")\r\n",
        "\r\n",
        "  for sent in tokenized_toolong.split(\"[SEP]\"):\r\n",
        "    print(f\"> {len(sent.strip().split(' ')) }\\ttokens:   {sent.strip()}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 318 sentences longer than 512 tokens.\n",
            "Longest sentences's length: 852\n",
            "\n",
            "Longest sentence (id 18797): \n",
            "  Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.\n",
            "\n",
            "Tokenized: \n",
            "  [CLS] when were slavic languages recognized as indo - european ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] slavic studies began as an almost exclusively linguistic and phil ##ological enterprise . [SEP] as early as 1833 , slavic languages were recognized as indo - european . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused85] [SEP]\n",
            "\n",
            "\n",
            "> 11\ttokens:   [CLS] when were slavic languages recognized as indo - european ?\n",
            "> 64\ttokens:   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] slavic studies began as an almost exclusively linguistic and phil ##ological enterprise .\n",
            "> 14\ttokens:   as early as 1833 , slavic languages were recognized as indo - european .\n",
            "> 852\ttokens:   [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [unused85]\n",
            "> 1\ttokens:   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wt_kI2O0c3u",
        "outputId": "496c0401-6b67-480e-ecb7-ea70b389f1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "for i in range(0, steps_per_epoch):\r\n",
        "  input = {'input_ids' : model_input['input_ids'][i:i+batch_size],\r\n",
        "           'input_mask' : model_input['input_mask'][i:i+batch_size],\r\n",
        "           'input_type_ids' : model_input['input_type_ids'][i:i+batch_size]\r\n",
        "           }\r\n",
        "  print(input['input_ids'])\r\n",
        "  bert_pretrained(input, training = True)\r\n",
        "  break\r\n",
        "\r\n",
        "# bert_pretrained(model_input, training = True)\r\n",
        "# for i in range(0, train_data_size):\r\n",
        "#   input = {'input_ids' : model_input['input_ids'][i],\r\n",
        "#            'input_mask' : model_input['input_mask'][i],\r\n",
        "#            'input_type_ids' : model_input['input_type_ids'][i]\r\n",
        "#            }\r\n",
        "#   bert_pretrained( input_ids = model_input['input_ids'][i], attention_mask = model_input['input_mask'][i], token_type_ids = model_input['input_type_ids'][i] , training = True)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  101  2000  3183 ...     0   515   541]\n",
            " [  101  2054  2003 ...     0   188   213]\n",
            " [  101  1996 13546 ...     0   279   296]\n",
            " ...\n",
            " [  101  2054  2820 ...     0     4    58]\n",
            " [  101  2054  2003 ...     0   466   516]\n",
            " [  101  1999  2054 ...     0   303   307]], shape=(32, 945), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-e5f5ccc62fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m            }\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mbert_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         )\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs_embeds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         )\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, mode, training)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/bert/modeling_tf_bert.py\u001b[0m in \u001b[0;36m_embedding\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, training)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# Instead of casting the variable as in most layers, cast the output, as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[0;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         result = _clip(\n\u001b[0;32m--> 138\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4811\u001b[0m     \u001b[0;31m# TODO(apassos) find a less bad way of detecting resource variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4812\u001b[0m     \u001b[0;31m# without introducing a circular dependency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4813\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4814\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4815\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    701\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 703\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,833] = 833 is not in [0, 512) [Op:ResourceGather]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGZ3ekP8mdEg"
      },
      "source": [
        "max(sentences_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}