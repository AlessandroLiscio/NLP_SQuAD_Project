{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQUAD_evaluating questions",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIya2vFoIquPaOi/OWuLkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliofortini/NLP_SQuAD_Project/blob/gpt/SQUAD_evaluating_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipI5dOLsUF_y"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b_Vs4qACfr3",
        "outputId": "dfacb36a-3d57-4bed-82be-4cddc9d0b3ec"
      },
      "source": [
        "import json\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!pip install nltk --upgrade\r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt') # if necessary...\r\n",
        "\r\n",
        "import string\r\n",
        "\r\n",
        "from nltk.translate import nist_score, bleu_score, meteor_score\r\n",
        "\r\n",
        "# demo\r\n",
        "#!/usr/bin/python\r\n",
        "!pip install transformers\r\n",
        "import transformers\r\n",
        "from transformers import BertTokenizer\r\n",
        "import tensorflow as tf\r\n",
        "tf.get_logger().setLevel('ERROR')\r\n",
        "transformers.logging.set_verbosity_error() # suppress tokenizer sentences' length warnings\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "#from utils import create_output_dict, preprocess_df, print_squad_sample, from_df_to_model_dict\r\n",
        "#from model import build_model\r\n",
        "import json\r\n",
        "import sys\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from nltk.translate import bleu_score, nist_score, meteor_score\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuFSjnaKT_Be"
      },
      "source": [
        "#Demo\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5HgwT9dUDPG"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzyOstTEMjwn"
      },
      "source": [
        "##Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmkO2WeoFPeH"
      },
      "source": [
        "def paragraph(text, max_width=80, indent=0):\r\n",
        "  if len(text) > max_width:\r\n",
        "    cut = max_width\r\n",
        "    while text[cut] != \" \": cut -= 1\r\n",
        "    return \" \"*indent + text[:cut].strip() + \"\\n\" + paragraph(text[cut:], max_width, indent=indent)\r\n",
        "  else:\r\n",
        "    return \" \"*indent + text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQgxcggtFdhW"
      },
      "source": [
        "def remove_tags(text):\r\n",
        "  return (text.\r\n",
        "          replace(\"[CTX]\", \"\").\r\n",
        "          replace(\"[QS]\", \"\").\r\n",
        "          replace(\"[QE]\", \"\").\r\n",
        "          strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otvT-jKl-6jY"
      },
      "source": [
        "stemmer = nltk.stem.porter.PorterStemmer()\r\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\r\n",
        "\r\n",
        "def stem_tokens(tokens):\r\n",
        "    return [stemmer.stem(item) for item in tokens]\r\n",
        "\r\n",
        "'''remove punctuation, lowercase, stem'''\r\n",
        "def normalize(text):\r\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\r\n",
        "\r\n",
        "def cosine_sim(text1, text2):\r\n",
        "    vectorizer = TfidfVectorizer(tokenizer=normalize)\r\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\r\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\r\n",
        "\r\n",
        "def jaccard(a, b):\r\n",
        "  set_a = set(normalize(a))\r\n",
        "  set_b = set(normalize(b))\r\n",
        "  return len(set_a.intersection(set_b)) / len(set_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWqd2MNjMlhl"
      },
      "source": [
        "## Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llC0uj6SvCid",
        "outputId": "0452c4d5-7635-4490-c316-0d6adedd98cf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bring_out', 'issue', 'print', 'publish', 'put_out', 'release', 'write'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTRTGSLzMrXM",
        "outputId": "5720d889-73ac-4aa4-b047-b8e731bef2ca"
      },
      "source": [
        "sample_id = 42\r\n",
        "#sample_context, sample_questions = list(questions.items())[sample_id]\r\n",
        "\r\n",
        "sample_context = \"Bidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.\"\r\n",
        "sample_questions = [\"What is BERT?\", \r\n",
        "                    \"What is bert based on?\", \r\n",
        "                    \"Who released BERT in last two years?\",\r\n",
        "                    \"Who published BERT in last two years?\",\r\n",
        "                    \"What company did the creator of bert work for?\",\r\n",
        "                    \"What does BERT stand for?\",\r\n",
        "                    \"When did Dante Alighieri die?\",\r\n",
        "                    \"Where is Alma Mater Studiorum Located?\",\r\n",
        "                    \"Why have I been sitting here from an hour?\"\r\n",
        "                    ]\r\n",
        "\r\n",
        "print(f\"Sample context:\\n{paragraph(remove_tags(sample_context), indent=2)}\\n\\nSample questions:\")\r\n",
        "for q in sample_questions:\r\n",
        "  print(\"-\", q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample context:\n",
            "  Bidirectional Encoder Representations from Transformers (BERT) is a\n",
            "  Transformer-based machine learning technique for natural language processing\n",
            "  (NLP) pre-training developed by Google. BERT was created and published in 2018\n",
            "  by Jacob Devlin and his colleagues from Google.\n",
            "\n",
            "Sample questions:\n",
            "- What is BERT?\n",
            "- What is bert based on?\n",
            "- Who released BERT in last two years?\n",
            "- Who published BERT in last two years?\n",
            "- What company did the creator of bert work for?\n",
            "- What does BERT stand for?\n",
            "- When did Dante Alighieri die?\n",
            "- Where is Alma Mater Studiorum Located?\n",
            "- Why have I been sitting here from an hour?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjrJ_3SU3nhY"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzYTTa5fgBNB"
      },
      "source": [
        "def get_synonims(word):\r\n",
        "  syn_set = set()\r\n",
        "  for syn in wordnet.synsets(word):\r\n",
        "    syn_set = syn_set.union(set(lemma.name() for lemma in syn.lemmas()))\r\n",
        "  return syn_set if syn_set != [] else {word}\r\n",
        "\r\n",
        "def reformulate_sentence(sentence, verbose=False):\r\n",
        "  alternatives = [sentence]\r\n",
        "  for word in sentence.split(\" \"):\r\n",
        "    if verbose: print(f\"\\n{word}...........\")\r\n",
        "    synonims = get_synonims(word)\r\n",
        "    for syn in synonims:\r\n",
        "      altern = sentence.replace(word, syn.lower().replace(\"_\", \" \"))\r\n",
        "      if verbose: print(altern)\r\n",
        "      alternatives.append(altern)\r\n",
        "  return alternatives\r\n",
        "\r\n",
        "def proposed(reference, sentence, return_alternative=False, verbose=False):\r\n",
        "  scores = []\r\n",
        "  if verbose: print(\"\\n\", sentence)\r\n",
        "  for alternative in reformulate_sentence(sentence):\r\n",
        "    score = jaccard(reference, alternative)\r\n",
        "    if verbose: print(score, alternative)\r\n",
        "    scores.append((score, alternative))\r\n",
        "  \r\n",
        "  if not return_alternative:\r\n",
        "    return max(scores)[0]\r\n",
        "  else:\r\n",
        "    return max(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2m9aT3XyBdz",
        "outputId": "0564448e-7da5-49f2-8288-ae197d1650fb"
      },
      "source": [
        "proposed(sample_context, \"what is bert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qg326n0Kqh8",
        "outputId": "0b82ac4b-bc0a-4972-fa25-674d47b316b5"
      },
      "source": [
        "print(paragraph(sample_context), \"\\n\\n\")\r\n",
        "print(\"{:<10}{:<10}{:<10}{:10}{:<10}\\n{:.>80}\".format(\"Jaccard\", \"Cosine\", \"METEOR\", \"Proposed\", \"Question\", \"\"))\r\n",
        "for question in sample_questions:\r\n",
        "  meteor = meteor_score.single_meteor_score(sample_context, question)\r\n",
        "  print(\"{:<10}{:<10}{:<10}{:<10}{}\".format(round(jaccard(sample_context, question), 3), \r\n",
        "                                      round(cosine_sim(sample_context, question), 3),\r\n",
        "                                      round(meteor, 3),\r\n",
        "                                      round(proposed(sample_context, question), 3),\r\n",
        "                                      question))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bidirectional Encoder Representations from Transformers (BERT) is a\n",
            "Transformer-based machine learning technique for natural language processing\n",
            "(NLP) pre-training developed by Google. BERT was created and published in 2018\n",
            "by Jacob Devlin and his colleagues from Google. \n",
            "\n",
            "\n",
            "Jaccard   Cosine    METEOR    Proposed  Question  \n",
            "................................................................................\n",
            "0.667     0.162     0.015     0.667     What is BERT?\n",
            "0.4       0.115     0.03      0.4       What is bert based on?\n",
            "0.286     0.094     0.03      0.429     Who released BERT in last two years?\n",
            "0.429     0.131     0.045     0.429     Who published BERT in last two years?\n",
            "0.222     0.081     0.015     0.333     What company did the creator of bert work for?\n",
            "0.4       0.115     0.015     0.4       What does BERT stand for?\n",
            "0.0       0.0       0.0       0.0       When did Dante Alighieri die?\n",
            "0.167     0.032     0.015     0.167     Where is Alma Mater Studiorum Located?\n",
            "0.111     0.052     0.015     0.2       Why have I been sitting here from an hour?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvDcMaxhZUed"
      },
      "source": [
        "## Proposed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_XmUR-V3ZJw"
      },
      "source": [
        "# Evaluation on the Test Set"
      ]
    }
  ]
}